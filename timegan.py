"""Time-series Generative Adversarial Networks (TimeGAN) Codebase.

Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, 
"Time-series Generative Adversarial Networks," 
Neural Information Processing Systems (NeurIPS), 2019.

Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks

Last updated Date: April 24th 2020
Code author: Jinsung Yoon (jsyoon0823@gmail.com)

-----------------------------

timegan.py

Note: Use original data as training set to generater synthetic data (time-series)
"""

# Necessary Packages
import os
import tensorflow as tf
from collections import OrderedDict
from tqdm import tqdm


os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}

import time
from datetime import datetime, timedelta

import numpy as np
from utils import extract_time, rnn_cell, random_generator, batch_generator, random_generator_alt, save_losses_csv


def timegan(ori_data, parameters, experiment_root_directory_name):
    """TimeGAN function.
  
  Use original data as training set to generater synthetic data (time-series)
  
  Args:
    - ori_data: original time-series data
    - parameters: TimeGAN network parameters
    
  Returns:
    - generated_data: generated time-series data
  """
    # Initialization on the Graph
    print("Initialization on the Graph")

    tf.reset_default_graph()
    sess = tf.Session()

    # Basic Parameters
    no, seq_len, dim = np.asarray(ori_data).shape

    # Maximum sequence length and each sequence length


    ori_time, max_seq_len = extract_time(ori_data)


    def MinMaxScaler(data):
        """Min-Max Normalizer.
    
    Args:
      - data: raw data
      
    Returns:
      - norm_data: normalized data
      - min_val: minimum values (for renormalization)
      - max_val: maximum values (for renormalization)
    """
        min_val = np.min(np.min(data, axis=0), axis=0)
        data = data - min_val

        max_val = np.max(np.max(data, axis=0), axis=0)
        norm_data = data / (max_val + 1e-7)

        return norm_data, min_val, max_val

    # Normalization
    ori_data, min_val, max_val = MinMaxScaler(ori_data)

    ## Build a RNN networks

    # Network Parameters
    hidden_dim = parameters['hidden_dim']
    num_layers = parameters['num_layer']
    iterations = parameters['iterations']
    batch_size = parameters['batch_size']
    module_name = parameters['module']
    z_dim = dim
    gamma = 1

    # Input place holders
    X = tf.placeholder(tf.float32, [None, max_seq_len, dim], name="myinput_x")
    Z = tf.placeholder(tf.float32, [None, max_seq_len, z_dim], name="myinput_z")
    T = tf.placeholder(tf.int32, [None], name="myinput_t")

    def embedder(X, T):
        """Embedding network between original feature space to latent space.
    
    Args:
      - X: input time-series features
      - T: input time information
      
    Returns:
      - H: embeddings
    """
        with tf.variable_scope("embedder", reuse=tf.AUTO_REUSE):
            e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])
            e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, X, dtype=tf.float32, sequence_length=T)
            H = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)
        return H

    def recovery(H, T):
        """Recovery network from latent space to original space.
    
    Args:
      - H: latent representation
      - T: input time information
      
    Returns:
      - X_tilde: recovered data
    """
        with tf.variable_scope("recovery", reuse=tf.AUTO_REUSE):
            r_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])
            r_outputs, r_last_states = tf.nn.dynamic_rnn(r_cell, H, dtype=tf.float32, sequence_length=T)
            X_tilde = tf.contrib.layers.fully_connected(r_outputs, dim, activation_fn=tf.nn.sigmoid)
        return X_tilde

    def generator(Z, T):
        """Generator function: Generate time-series data in latent space.
    
    Args:
      - Z: random variables
      - T: input time information
      
    Returns:
      - E: generated embedding
    """
        with tf.variable_scope("generator", reuse=tf.AUTO_REUSE):
            e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])
            e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, Z, dtype=tf.float32, sequence_length=T)
            E = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)
        return E

    def supervisor(H, T):
        """Generate next sequence using the previous sequence.
    
    Args:
      - H: latent representation
      - T: input time information

    Returns:
      - S: generated sequence based on the latent representations generated by the generator
    """
        with tf.variable_scope("supervisor", reuse=tf.AUTO_REUSE):
            e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers - 1)])
            e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, H, dtype=tf.float32, sequence_length=T)
            S = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)
        return S

    def discriminator(H, T):
        """Discriminate the original and synthetic time-series data.
    
    Args:
      - H: latent representation
      - T: input time information
      
    Returns:
      - Y_hat: classification results between original and synthetic time-series
    """
        with tf.variable_scope("discriminator", reuse=tf.AUTO_REUSE):
            d_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])
            d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, H, dtype=tf.float32, sequence_length=T)
            Y_hat = tf.contrib.layers.fully_connected(d_outputs, 1, activation_fn=None)
        return Y_hat

    # Embedder & Recovery
    H = embedder(X, T)
    X_tilde = recovery(H, T)

    # Generator
    E_hat = generator(Z, T)
    H_hat = supervisor(E_hat, T)
    H_hat_supervise = supervisor(H, T)

    # Synthetic data
    X_hat = recovery(H_hat, T)

    # Discriminator
    Y_fake = discriminator(H_hat, T)
    Y_real = discriminator(H, T)
    Y_fake_e = discriminator(E_hat, T)

    # Variables
    e_vars = [v for v in tf.trainable_variables() if v.name.startswith('embedder')]
    r_vars = [v for v in tf.trainable_variables() if v.name.startswith('recovery')]
    g_vars = [v for v in tf.trainable_variables() if v.name.startswith('generator')]
    s_vars = [v for v in tf.trainable_variables() if v.name.startswith('supervisor')]
    d_vars = [v for v in tf.trainable_variables() if v.name.startswith('discriminator')]

    # Discriminator loss
    D_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_real), Y_real)
    D_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake), Y_fake)
    D_loss_fake_e = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake_e), Y_fake_e)
    D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e

    # Generator loss
    # 1. Adversarial loss
    G_loss_U = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake), Y_fake)
    G_loss_U_e = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake_e), Y_fake_e)

    # 2. Supervised loss
    G_loss_S = tf.losses.mean_squared_error(H[:, 1:, :], H_hat_supervise[:, :-1, :])

    # 3. Two Momments
    G_loss_V1 = tf.reduce_mean(tf.abs(tf.sqrt(tf.nn.moments(X_hat, [0])[1] + 1e-6) - tf.sqrt(
        tf.nn.moments(X, [0])[1] + 1e-6)))  # diferencia de varianzas
    G_loss_V2 = tf.reduce_mean(
        tf.abs((tf.nn.moments(X_hat, [0])[0]) - (tf.nn.moments(X, [0])[0])))  # diferencia de medias

    G_loss_V = G_loss_V1 + G_loss_V2

    # 4. Summation
    G_loss = G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100 * G_loss_V

    # Embedder network loss
    E_loss_T0 = tf.losses.mean_squared_error(X, X_tilde)
    E_loss0 = 10 * tf.sqrt(E_loss_T0)
    E_loss = E_loss0 + 0.1 * G_loss_S

    # optimizer
    E0_solver = tf.train.AdamOptimizer().minimize(E_loss0, var_list=e_vars + r_vars)
    E_solver = tf.train.AdamOptimizer().minimize(E_loss, var_list=e_vars + r_vars)
    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=d_vars)
    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=g_vars + s_vars)
    GS_solver = tf.train.AdamOptimizer().minimize(G_loss_S, var_list=g_vars + s_vars)

    ## TimeGAN training
    print('Comienza entrenamiento')
    sess.run(tf.global_variables_initializer())

    # 1. Embedding network training H->espacio de caracter√≠sticas latente
    print('Start Embedding Network Training')
    progress_bar_embedding = tqdm(range(iterations))
    for itt in progress_bar_embedding:
        # Set mini-batch
        X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
        # Train embedder
        _, step_e_loss = sess.run([E0_solver, E_loss_T0], feed_dict={X: X_mb, T: T_mb})
        progress_bar_embedding.set_description(f'e_loss: {np.sqrt(step_e_loss):.4f}')
        # Checkpoint
        # if itt % 100 == 0:
        #     end = time.time()
        #     eta_secs = (end - start) * (iterations - itt) / 100
        #     now = datetime.now()
        #     eta_datetime = now + timedelta(seconds=eta_secs)
        #     print('step: ' + str(itt) + '/' + str(iterations) + ', e_loss: ' + str(np.round(np.sqrt(step_e_loss), 4)))
        #     if itt != 0:
        #         print('\tElapsed time: ', round(end - start, 1), " sgs.", "ETA:", round(eta_secs / 60, 1), "mins.",
        #               eta_datetime)
        #     start = time.time()

    print('Finish Embedding Network Training. step: ' + str(itt) + '/' + str(iterations) + ', Final e_loss: ' + str(
        np.round(np.sqrt(step_e_loss), 4)))

    # 2. Training only with supervised loss
    print('Start Training with Supervised Loss Only')
    progress_bar_supervised = tqdm(range(iterations))
    for itt in progress_bar_supervised:
        # Set mini-batch
        X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
        # Random vector generation
        Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
        # Train generator
        _, step_g_loss_s = sess.run([GS_solver, G_loss_S], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})
        # Checkpoint
        progress_bar_supervised.set_description(f' s_loss: {np.sqrt(step_g_loss_s):.4f}')
        # if itt % 100 == 0:
        #     end = time.time()
        #     eta_secs = (end - start) * (iterations - itt) / 100
        #     now = datetime.now()
        #     eta_datetime = now + timedelta(seconds=eta_secs)
        #     print('step: ' + str(itt) + '/' + str(iterations) + ', s_loss: ' + str(np.round(np.sqrt(step_g_loss_s), 4)))
        #     if itt != 0:
        #         print('\tElapsed time:', round(end - start, 1), " sgs.", "ETA:", round(eta_secs / 60, 1), " mins.",
        #               eta_datetime)
        #     start = time.time()

    print('Finish Training with Supervised Loss Only. step: ' + str(itt) + '/' + str(
        iterations) + ', Final s_loss: ' + str(np.round(np.sqrt(step_g_loss_s), 4)))

    # 3. Joint Training
    print('Start Joint Training')
    losses = OrderedDict()
    losses['d_loss'] = []
    losses['e_loss_t0'] = []
    losses['g_loss_s'] = []
    losses['g_loss_u'] = []
    losses['g_loss_v'] = []


    progress_bar_joint = tqdm(range(iterations))
    for itt in progress_bar_joint:
        # Generator training (twice more than discriminator training)
        for kk in range(2):
            # Set mini-batch
            X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
            # Random vector generation
            Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
            # Train generator
            _, step_g_loss_u, step_g_loss_s, step_g_loss_v = sess.run([G_solver, G_loss_U, G_loss_S, G_loss_V],
                                                                      feed_dict={Z: Z_mb, X: X_mb, T: T_mb})
            # Train embedder
            _, step_e_loss_t0 = sess.run([E_solver, E_loss_T0], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})

        # Discriminator training
        # Set mini-batch
        X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
        # Random vector generation
        Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
        # Check discriminator loss before updating
        check_d_loss = sess.run(D_loss, feed_dict={X: X_mb, T: T_mb, Z: Z_mb})
        # Train discriminator (only when the discriminator does not work well)
        if (check_d_loss > 0.15):
            _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, Z: Z_mb})
        else:
            step_d_loss = check_d_loss


        losses['d_loss'].append(step_d_loss)
        losses['g_loss_u'].append(step_g_loss_u)
        losses['g_loss_s'].append(step_g_loss_s)
        losses['g_loss_v'].append(step_g_loss_v)
        losses['e_loss_t0'].append(step_e_loss_t0)
        progress_bar_joint.set_description(f'd_loss: {step_d_loss:.4f}. g_loss_u: {step_g_loss_u:.4f}')

        if itt % 50 == 0:
            generate_data(z_dim, max_seq_len,T, X, X_hat, Z, max_val, min_val, parameters['n_samples'], ori_data, ori_time, sess, f'{experiment_root_directory_name}/epoch_{itt}/generated_data/')

    # Print multiple checkpoints
        # if itt % 100 == 0:
        #     #generate_data(T, X, X_hat, Z, Z_mb, max_val, min_val, parameters['n_samples'], ori_data, ori_time, sess,
        #     #              f'{experiment_root_directory_name}/epoch_{itt}/generated_data/')
        #     end = time.time()
        #     eta_secs = (end - start) * (iterations - itt) / 100
        #     now = datetime.now()
        #     eta_datetime = now + timedelta(seconds=eta_secs)
        #     print('step: ' + str(itt) + '/' + str(iterations) +
        #           ', d_loss: ' + str(np.round(step_d_loss, 4)) +
        #           ', g_loss_u: ' + str(np.round(step_g_loss_u, 4)) +
        #           ', g_loss_s: ' + str(np.round(np.sqrt(step_g_loss_s), 4)) +
        #           ', g_loss_v: ' + str(np.round(step_g_loss_v, 4)) +
        #           ', e_loss_t0: ' + str(np.round(np.sqrt(step_e_loss_t0), 4)))
        #     if itt != 0:
        #         print('\tElapsed time: ', round(end - start, 1), " sgs.", "ETA: ", round(eta_secs / 60, 1), " mins.",
        #               eta_datetime)
        #     start = time.time()

    print('Finish Joint Training. step: ' + str(itt) + '/' + str(iterations) +
          ', Final d_loss: ' + str(np.round(step_d_loss, 4)) +
          ', g_loss_u: ' + str(np.round(step_g_loss_u, 4)) +
          ', g_loss_s: ' + str(np.round(np.sqrt(step_g_loss_s), 4)) +
          ', g_loss_v: ' + str(np.round(step_g_loss_v, 4)) +
          ', e_loss_t0: ' + str(np.round(np.sqrt(step_e_loss_t0), 4)))


    ## Synthetic data generation
    # Generation Parameters
    # print('Start synthetic generation')
    # print('\tStart Random generator')
    # Z_mb = random_generator_alt(parameters['n_samples'], z_dim, ori_time, max_seq_len)
    # print('\nRandom generator finished')
    # generated_data = generate_data(T, X, X_hat, Z, Z_mb, max_val, min_val, parameters['n_samples'], ori_data, ori_time, sess, f'{experiment_root_directory_name}/generated_data')
    # Generation Parameters

    print('Start synthetic generation')

    generated_data = generate_data(z_dim, max_seq_len,T, X, X_hat, Z, max_val, min_val, parameters['n_samples'], ori_data, ori_time, sess, f'{experiment_root_directory_name}/generated_data/')

    print('Finished synthetic generation')

    inputs = {"myinput_x": X, "myinput_z": Z, "myinput_t":T}
    outputs = {"x_hat": X_hat}
    save_model(experiment_root_directory_name, inputs, outputs, sess, iterations)
    save_losses_csv(experiment_root_directory_name, losses)
    #
    # generated_data = list()
    # for i in range(n_samples):
    #     temp = generated_data_curr[i, :ori_time[i], :]
    #     generated_data.append(temp)
    # # Renormalization
    # generated_data = generated_data * max_val
    # generated_data = generated_data + min_val

    return generated_data



def generate_data(z_dim, max_seq_len, T, X, X_hat, Z, max_val, min_val, n_samples, ori_data, ori_time, sess, save_generated_data_dir):
    Z_mb = random_generator_alt(n_samples, z_dim, ori_time, max_seq_len)
    generated_data_curr = sess.run(X_hat, feed_dict={Z: Z_mb, X: ori_data, T: ori_time[:n_samples]})
    generated_data = list()
    for i in range(n_samples):
        temp = generated_data_curr[i, :ori_time[i], :]
        generated_data.append(temp)
    # Renormalization
    generated_data = generated_data * max_val
    generated_data = generated_data + min_val

    generated_data_np_array = np.asarray(generated_data)
    os.makedirs(save_generated_data_dir, exist_ok=True)

    i = 0
    for generated_sample in generated_data_np_array:
        np.savetxt(save_generated_data_dir + "sample_" + str(i) + ".csv", generated_sample, delimiter=",", fmt='%f')
        i = i + 1
    print(f'Generated {n_samples} samples.')
    return generated_data


def save_model(experiment_root_directory_name, inputs, outputs, sess, iteration=None):
    saved_model_directory = f'{experiment_root_directory_name}/models/'
    tf.saved_model.simple_save(sess, saved_model_directory, inputs, outputs)
    print(f'Model saved at {saved_model_directory}')
